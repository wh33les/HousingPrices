{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from prince import MCA\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with the modified data types\n",
    "with open(\"../raw_data/dtypes.json\", \"r\") as file:\n",
    "    dtypes = json.load(file)\n",
    "df_train = pd.read_csv(\"../raw_data/exp_train.csv\", dtype=dtypes)\n",
    "df_test = pd.read_csv(\"../raw_data/exp_test.csv\", dtype=dtypes)\n",
    "# df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the random seed in a log file then generate the kfold splits\n",
    "rs_log = open(\"rs_log.txt\", \"a\")\n",
    "rs_log.write(f\"\\n{datetime.now()}, rs = {randint(0,1000)}\")\n",
    "rs_log.close()\n",
    "with open(\"rs_log.txt\", \"r\") as rs_log:\n",
    "    rs = int(rs_log.readlines()[-1].split(\" \")[-1])\n",
    "\n",
    "# # For a specific random seed\n",
    "# rs = 393\n",
    "\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=rs)\n",
    "kfold_splits = kfold.split(df_train)\n",
    "\n",
    "# Name the splits\n",
    "dfs_tt, dfs_ho = [], []\n",
    "for train_index, test_index in kfold_splits:\n",
    "    dfs_tt.append(df_train.iloc[train_index])\n",
    "    dfs_ho.append(df_train.iloc[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute `GarageYrBlt`\n",
    "for i in range(n_splits):\n",
    "    yr_impute = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "    dfs_tt[i].loc[:, \"GarageYrBlt\"] = pd.DataFrame(\n",
    "        yr_impute.fit_transform(dfs_tt[i][[\"GarageYrBlt\"]])\n",
    "    )\n",
    "    dfs_ho[i].loc[:, \"GarageYrBlt\"] = pd.DataFrame(\n",
    "        yr_impute.transform(dfs_ho[i][[\"GarageYrBlt\"]])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines for the numerical features\n",
    "num_med_cols = [\n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",\n",
    "    \"YearBuilt\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"BedroomAbvGr\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"YrSold\",\n",
    "]\n",
    "num_mode_cols = [\n",
    "    \"LandSlope\",\n",
    "    \"LotShape\",\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"GarageFinish\",\n",
    "    \"BsmtCond\",\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"Functional\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PavedDrive\",\n",
    "    \"Fence\",\n",
    "    \"PoolQC\",  # (ordinals)\n",
    "    \"YearRemodAdd\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "]\n",
    "len_num_cols = len(num_med_cols) + len(num_mode_cols)\n",
    "\n",
    "pipe_num_med = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))])  # ,\n",
    "# IterativeImputer(estimator=BayesianRidge(), initial_strategy='median'))])\n",
    "#  ('range', StandardScaler())])#MinMaxScaler())])\n",
    "\n",
    "pipe_num_mode = Pipeline(\n",
    "    [(\"impute\", KNNImputer())]\n",
    ")  # SimpleImputer(strategy='most_frequent'))])#,\n",
    "# IterativeImputer(estimator=BayesianRidge(), initial_strategy='most_frequent'))])\n",
    "#  ('range', StandardScaler())])#MinMaxScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = (\n",
    "    df_train.select_dtypes(include=\"object\").drop(columns=[\"Id\"]).columns.to_list()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# OHE first to get the max number of components\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "\n",
    "new_cat_cols = []\n",
    "\n",
    "\n",
    "for i in range(n_splits):\n",
    "\n",
    "    id_tt = dfs_tt[i][\"Id\"]\n",
    "\n",
    "    id_ho = dfs_ho[i][\"Id\"]\n",
    "\n",
    "    ohe_transformed_tt = pd.DataFrame(ohe.fit_transform(dfs_tt[i][cat_cols]).toarray())\n",
    "\n",
    "    ohe_transformed_ho = pd.DataFrame(ohe.transform(dfs_ho[i][cat_cols]).toarray())\n",
    "\n",
    "    prices_tt = dfs_tt[i][\"SalePrice\"]\n",
    "\n",
    "    prices_ho = dfs_ho[i][\"SalePrice\"]\n",
    "\n",
    "    cols1 = num_med_cols + num_mode_cols\n",
    "\n",
    "    cols2 = ohe_transformed_tt.columns.to_list()\n",
    "\n",
    "    cols3 = [\"SalePrice\"]\n",
    "\n",
    "    dfs_tt[i] = pd.concat(\n",
    "        [\n",
    "            id_tt.reset_index(drop=True),\n",
    "            dfs_tt[i][cols1].reset_index(drop=True),\n",
    "            ohe_transformed_tt.reset_index(drop=True),\n",
    "            prices_tt.reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    dfs_tt[i].columns = [\"Id\"] + cols1 + [str(x) for x in cols2] + cols3\n",
    "\n",
    "    dfs_ho[i] = pd.concat(\n",
    "        [\n",
    "            id_ho.reset_index(drop=True),\n",
    "            dfs_ho[i][cols1].reset_index(drop=True),\n",
    "            ohe_transformed_ho.reset_index(drop=True),\n",
    "            prices_ho.reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    dfs_ho[i].columns = [\"Id\"] + cols1 + [str(x) for x in cols2] + cols3\n",
    "\n",
    "    new_cat_cols.append([str(x) for x in cols2])\n",
    "\n",
    "\n",
    "\n",
    "# Make the cat cols pipeline\n",
    "\n",
    "\n",
    "\n",
    "pipe_cat = MCA(n_components=len(new_cat_cols[i]) - 1, one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipelines (tried ColumnTransformer and it was problematic)\n",
    "n_comps_list = []\n",
    "for i in range(n_splits):\n",
    "    tt_num_med_transf = pd.DataFrame(\n",
    "        pipe_num_med.fit_transform(dfs_tt[i][num_med_cols])\n",
    "    )\n",
    "    ho_num_med_transf = pd.DataFrame(pipe_num_med.transform(dfs_ho[i][num_med_cols]))\n",
    "    tt_num_mode_transf = pd.DataFrame(\n",
    "        pipe_num_mode.fit_transform(dfs_tt[i][num_mode_cols])\n",
    "    )\n",
    "    ho_num_mode_transf = pd.DataFrame(pipe_num_mode.transform(dfs_ho[i][num_mode_cols]))\n",
    "    tt_cat_tranf = pd.DataFrame(pipe_cat.fit_transform(dfs_tt[i][new_cat_cols[i]]))\n",
    "    ho_cat_tranf = pd.DataFrame(pipe_cat.transform(dfs_ho[i][new_cat_cols[i]]))\n",
    "    dfs_tt[i] = pd.concat(\n",
    "        [\n",
    "            dfs_tt[i][\"Id\"],\n",
    "            tt_num_med_transf,\n",
    "            tt_num_mode_transf,\n",
    "            tt_cat_tranf,  # dfs_tt[i][new_cat_cols[i]],\n",
    "            dfs_tt[i][\"SalePrice\"],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    dfs_ho[i] = pd.concat(\n",
    "        [\n",
    "            dfs_ho[i][\"Id\"],\n",
    "            ho_num_med_transf,\n",
    "            ho_num_mode_transf,\n",
    "            ho_cat_tranf,  # dfs_ho[i][new_cat_cols[i]],\n",
    "            dfs_ho[i][\"SalePrice\"],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Determine the number of components to keep from MCA\n",
    "    exp_var = 0.8\n",
    "    eigenvalues = pipe_cat.eigenvalues_\n",
    "    total_inertia = eigenvalues.sum()\n",
    "    evr_list = eigenvalues / total_inertia\n",
    "    exp_var_sum = 0\n",
    "    n_comp = 0\n",
    "    for j in range(len(evr_list)):\n",
    "        exp_var_sum = exp_var_sum + evr_list[j]\n",
    "        if exp_var_sum <= exp_var:\n",
    "            n_comp += 1\n",
    "        else:\n",
    "            break\n",
    "    n_comps_list.append(n_comp)\n",
    "\n",
    "# Cut the extraneous features in the categorical dataframe\n",
    "# Standardizing number of columns across folds\n",
    "min_cols = 500\n",
    "for i in range(5):\n",
    "    min_cols = min(min_cols, dfs_tt[i].shape[1], dfs_ho[i].shape[1])\n",
    "\n",
    "n_cols = 1 + len_num_cols + max(n_comps_list)  # (min_cols-1-len_num_cols)\n",
    "for i in range(n_splits):\n",
    "    dfs_tt[i] = dfs_tt[i].iloc[:, list(range(n_cols)) + [-1]]\n",
    "    dfs_tt[i].columns = (\n",
    "        [\"Id\"]\n",
    "        + num_med_cols\n",
    "        + num_mode_cols\n",
    "        + list(range(max(n_comps_list)))\n",
    "        + [\"SalePrice\"]\n",
    "    )\n",
    "    dfs_ho[i] = dfs_ho[i].iloc[:, list(range(n_cols)) + [-1]]\n",
    "    dfs_ho[i].columns = (\n",
    "        [\"Id\"]\n",
    "        + num_med_cols\n",
    "        + num_mode_cols\n",
    "        + list(range(max(n_comps_list)))\n",
    "        + [\"SalePrice\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data\n",
    "for i in range(n_splits):\n",
    "    dfs_tt[i].to_csv(f\"preproc_tt_fold_{i}.csv\", index=False)\n",
    "    dfs_ho[i].to_csv(f\"preproc_ho_fold_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 142)\n",
      "(292, 142)\n",
      "(1168, 142)\n",
      "(292, 142)\n",
      "(1168, 142)\n",
      "(292, 142)\n",
      "(1168, 142)\n",
      "(292, 142)\n",
      "(1168, 142)\n",
      "(292, 142)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape for input size in the neural network\n",
    "for i in range(5):\n",
    "\n",
    "    print(dfs_tt[i].shape)\n",
    "\n",
    "    print(dfs_ho[i].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing_prices_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
