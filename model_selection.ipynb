{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor \n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import regr_nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "\n",
    "dfs_tt, dfs_ho = [], []\n",
    "data_dict = {'dfs_tt':dfs_tt, 'dfs_ho':dfs_ho}\n",
    "for i in range(5):\n",
    "    for key in data_dict:\n",
    "        data_dict[key].append(pd.read_csv(f\"preprocessing/preproc_{key.split(\"_\")[-1]}_fold_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normality\n",
    "\n",
    "linregr = LinearRegression()\n",
    "for i in range(5):\n",
    "    linregr.fit(dfs_tt[i].iloc[:,1:-1], dfs_tt[i].iloc[:,-1])\n",
    "    y_pred = linregr.predict(dfs_ho[i].iloc[:,1:-1])      \n",
    "    resids = dfs_ho[i].iloc[:,-1] - y_pred\n",
    "    \n",
    "    # Create a scatter plot of residuals\n",
    "   \n",
    "    plt.scatter(y_pred, resids)\n",
    "    plt.xlabel(f\"Predicted Values for fold #{i}\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residual Plot for fold #{i}\")\n",
    "    plt.axhline(y=0, color='b', linestyle='-')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "baseline = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree 2 polynomial with regularization models\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "l1ratios = np.linspace(start=0, stop=1, num=20, endpoint=False)\n",
    "\n",
    "deg2_ridge = Pipeline([('poly2', PolynomialFeatures(include_bias=False)),\n",
    "                      ('ridge', RidgeCV(alphas=alphas, cv=5))])\n",
    "deg2_lasso = Pipeline([('poly2', PolynomialFeatures(include_bias=False)),\n",
    "                      ('lasso', LassoCV(alphas=alphas))])\n",
    "deg2_elasticnet = Pipeline([('poly2', PolynomialFeatures(include_bias=False)),\n",
    "                      ('elasticnet', ElasticNetCV(alphas=alphas, l1_ratio=l1ratios, cv=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble learning\n",
    "\n",
    "randomforest = RandomForestRegressor()\n",
    "adaboost = AdaBoostRegressor()\n",
    "xgradboost = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [baseline, deg2_ridge, deg2_lasso, deg2_elasticnet, randomforest, adaboost, xgradboost]#, neuralnetwork]\n",
    "# rmses = []\n",
    "# for model in models:\n",
    "#     model.fit(dfs_tt[0].iloc[:,1:-1], dfs_tt[0].iloc[:,-1])\n",
    "#     y_pred = model.predict(dfs_ho[0].iloc[:,1:-1])\n",
    "#     rmses.append(np.sqrt(mean_squared_error(dfs_ho[0].iloc[:,-1], y_pred)))\n",
    "# print(rmses)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leyjf\\Dropbox\\github\\HousingPrices\\housing_prices_venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1168])) that is different to the input size (torch.Size([1168, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred at epoch 0: tensor([[0.1601],\n",
      "        [0.2145],\n",
      "        [0.1631],\n",
      "        ...,\n",
      "        [0.2463],\n",
      "        [0.0976],\n",
      "        [0.1700]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 0: 38996500480.0\n",
      "y_pred at epoch 1000: tensor([[181151.5938],\n",
      "        [184378.1094],\n",
      "        [179017.0312],\n",
      "        ...,\n",
      "        [179981.7969],\n",
      "        [176410.5938],\n",
      "        [182337.3906]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 1000: 6279001600.0\n",
      "y_pred at epoch 2000: tensor([[180861.7969],\n",
      "        [182431.5938],\n",
      "        [179932.8750],\n",
      "        ...,\n",
      "        [181623.4531],\n",
      "        [178180.7969],\n",
      "        [181787.0312]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 2000: 6276279808.0\n",
      "y_pred at epoch 3000: tensor([[180757.9688],\n",
      "        [181682.4531],\n",
      "        [180400.6719],\n",
      "        ...,\n",
      "        [181782.1094],\n",
      "        [179257.4844],\n",
      "        [181462.7812]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 3000: 6275366912.0\n",
      "y_pred at epoch 4000: tensor([[180744.5625],\n",
      "        [181353.8594],\n",
      "        [180654.4688],\n",
      "        ...,\n",
      "        [181666.3594],\n",
      "        [179858.7500],\n",
      "        [181267.1875]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 4000: 6274961920.0\n",
      "y_pred at epoch 5000: tensor([[180763.3906],\n",
      "        [181190.5938],\n",
      "        [180794.0156],\n",
      "        ...,\n",
      "        [181513.3125],\n",
      "        [180202.7656],\n",
      "        [181141.5000]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 5000: 6274760192.0\n",
      "y_pred at epoch 6000: tensor([[180789.7344],\n",
      "        [181100.0000],\n",
      "        [180869.7188],\n",
      "        ...,\n",
      "        [181377.9375],\n",
      "        [180411.0000],\n",
      "        [181057.1719]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 6000: 6274655232.0\n",
      "y_pred at epoch 7000: tensor([[180814.8125],\n",
      "        [181044.9219],\n",
      "        [180909.3594],\n",
      "        ...,\n",
      "        [181269.8906],\n",
      "        [180544.5938],\n",
      "        [180999.5781]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 7000: 6274597888.0\n",
      "y_pred at epoch 8000: tensor([[180836.0938],\n",
      "        [181008.9375],\n",
      "        [180928.6250],\n",
      "        ...,\n",
      "        [181186.8906],\n",
      "        [180634.6406],\n",
      "        [180960.1875]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 8000: 6274566144.0\n",
      "y_pred at epoch 9000: tensor([[180853.3438],\n",
      "        [180984.0625],\n",
      "        [180936.5156],\n",
      "        ...,\n",
      "        [181124.0000],\n",
      "        [180697.6875],\n",
      "        [180933.3750]], grad_fn=<AddmmBackward0>)\n",
      "loss at epoch 9000: 6274549248.0\n",
      "                 0\n",
      "0    180583.890625\n",
      "1    180785.984375\n",
      "2    180818.015625\n",
      "3    181147.203125\n",
      "4    180982.984375\n",
      "..             ...\n",
      "287  180995.671875\n",
      "288  180984.328125\n",
      "289  180870.656250\n",
      "290  181013.250000\n",
      "291  180832.656250\n",
      "\n",
      "[292 rows x 1 columns]\n",
      "rmse = 80189.30649744575\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "\n",
    "neuralnetwork = regr_nn.RegressionNN(input_size=318, hidden_size = 8, output_size=1)\n",
    "neuralnetwork.fit(dfs_tt[0].iloc[:,1:-1], dfs_tt[0].iloc[:,-1])\n",
    "y_pred = pd.DataFrame(neuralnetwork.predict(dfs_ho[0].iloc[:,1:-1]))#.numpy())\n",
    "print(y_pred) \n",
    "print(f'rmse = {(np.sqrt(mean_squared_error(dfs_ho[0].iloc[:,-1], y_pred)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sample import MyModel\n",
    "\n",
    "# # Example usage\n",
    "# model = MyModel(input_size=10, hidden_size=20, output_size=1)\n",
    "# X = torch.randn(100, 10)\n",
    "# y = torch.randn(100, 1)\n",
    "\n",
    "# model.fit(X, y, epochs=1000)\n",
    "# predictions = model.predict(X)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sesample import MyNeuralNet\n",
    "# model = MyNeuralNet()\n",
    "# X_train = dfs_tt[0].iloc[:,1:-1]\n",
    "# y_train = dfs_tt[0].iloc[:,-1]\n",
    "# X_test = dfs_ho[0].iloc[:,1:-1]\n",
    "# model.fit(X_train, y_train)\n",
    "# pred = model.predict(X_test)\n",
    "# pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing_prices_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
